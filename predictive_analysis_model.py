# -*- coding: utf-8 -*-
"""Predictive_Analysis_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WBZSYW2MxiYH-DDXzKd9_nP0zok7VYk5

# **Hey there! Welcome to my project for predicting a student's academic stream after 10th.**
### The idea is to use their past grades, hobbies, and a few other details to see if we can guess whether they'll lean towards Science, Commerce, or Arts.
### We'll calculate the probability percentage of each stream, which the student is likely to choose.
"""

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import RandomizedSearchCV, train_test_split
from sklearn.metrics import accuracy_score, classification_report, log_loss
from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import warnings
import joblib

# Ignore warnings for cleaner output
warnings.filterwarnings('ignore')

# Defining where my data is and what I'm trying to predict.

TRAIN_FILE = 'Dataset_1.csv'
TEST_FILE = 'Dataset_2.csv'
TARGET_COLUMN = 'target_stream' # This is what we want to predict

# It's important to make sure these names perfectly match the columns in the CSV files.
# These are the straightforward categorical features.
CATEGORICAL_FEATURES = ['gender', 'family_background']
# Hobbies are already one-hot encoded in my dataset (1 for yes, 0 for no).
HOBBY_FEATURES = ['hobby_coding', 'hobby_music', 'hobby_art', 'hobby_painting_and_drama', 'hobby_sports', 'hobby_debate', 'hobby_reading']
# And here are all the numerical features, mostly grades from school.
NUMERICAL_FEATURES = [
    'math_5', 'science_5', 'social_science_5', 'english_5', 'second_language_5', # Grade 5
    'math_6', 'science_6', 'social_science_6', 'english_6', 'second_language_6', # Grade 6
    'math_7', 'science_7', 'social_science_7', 'english_7', 'second_language_7', # Grade 7
    'math_8', 'science_8', 'social_science_8', 'english_8', 'second_language_8', # Grade 8
    'math_9', 'science_9', 'social_science_9', 'english_9', 'second_language_9', # Grade 9
    'math_10', 'science_10', 'social_science_10', 'english_10', 'second_language_10', # Grade 10
    'avg_math_8_10', 'avg_science_8_10', 'avg_social_science_8_10', 'avg_english_8_10', 'avg_second_language_8_10', # Some averages I calculated
    'science_growth', 'consistency_growth' # A couple of growth metrics I thought might be interesting
]
# Let's just create one big list of all the features we'll be using.
ALL_FEATURES = CATEGORICAL_FEATURES + HOBBY_FEATURES + NUMERICAL_FEATURES

print("All libraries are imported and our settings are ready to go!")

# Loading the Data
# We'll load our datasets. I've got one for training the model and one for testing it.
try:
    df_train = pd.read_csv(TRAIN_FILE)
    df_test = pd.read_csv(TEST_FILE)
    print(f"\nAwesome, the training data is loaded. We've got {df_train.shape[0]} students in there.")
    print(f"And the testing data is loaded too, with {df_test.shape[0]} students.")

    # I'm making copies of the original data so I can add my predictions to them later without messing up the originals.
    df_train_orig = df_train.copy()
    df_test_orig = df_test.copy()

except FileNotFoundError:
    print(f"\n I couldn't find '{TRAIN_FILE}' or '{TEST_FILE}'.")
    print("Could you make sure they're in the same folder as this script?")
    exit() # Can't do much without the data, so let's stop here.

# Let's have a quick peek at what the data looks like.
print("\nHere's a little sample of the training data:")
print(df_train.head())
print("\nAnd a sample of the testing data:")
print(df_test.head())

# Step 1: Getting the Data Ready (Preprocessing)
print("\n Starting Data Prep and Cleanup")

# Let's get a feel for the structure of the data.
print("\nLet's see what we're working with in the training data...")
df_train.info()
print("\nAnd in the testing data...")
df_test.info()

# Now for a quick health check. Any missing values?
print("\nMissing values in the training set:")
print(df_train.isnull().sum().sort_values(ascending=False))
print("\nMissing values in the testing set:")
print(df_test.isnull().sum().sort_values(ascending=False))

# A common strategy is to fill in missing numbers with the median and missing categories with the most frequent one.
# I'll use the values from the training data to fill both datasets to avoid any data leakage!
if df_train[NUMERICAL_FEATURES].isnull().sum().sum() > 0:
    print("\nLooks like we have some missing numbers. Filling them in with the median...")
    for col in NUMERICAL_FEATURES:
        median_val = df_train[col].median()
        df_train[col].fillna(median_val, inplace=True)
        df_test[col].fillna(median_val, inplace=True)
    print("Done! All patched up.")

if df_train[CATEGORICAL_FEATURES].isnull().sum().sum() > 0:
    print("\nFound some missing categories. Filling them with the most common value...")
    for col in CATEGORICAL_FEATURES:
        mode_val = df_train[col].mode()[0]
        df_train[col].fillna(mode_val, inplace=True)
        df_test[col].fillna(mode_val, inplace=True)
    print("Categorical data is all filled in.")


#  Next step: Separate Features (X) and Target (y)
# Ensure TARGET_COLUMN exists
if TARGET_COLUMN not in df_train.columns or TARGET_COLUMN not in df_test.columns:
    print(f"\nError! Can't find the target column '{TARGET_COLUMN}' in one of the files.")
    exit()

#Verify all defined features exist in the dataframe
missing_features = [f for f in ALL_FEATURES if f not in df_train.columns]
if missing_features:
    print(f"\nHeads up! These features are defined in my list but missing from the data: {missing_features}")
    print("No worries, I'll just use the features that are actually present.")
    ALL_FEATURES = [f for f in ALL_FEATURES if f in df_train.columns]
    CATEGORICAL_FEATURES = [f for f in CATEGORICAL_FEATURES if f in df_train.columns]
    HOBBY_FEATURES = [f for f in HOBBY_FEATURES if f in df_train.columns]
    NUMERICAL_FEATURES = [f for f in NUMERICAL_FEATURES if f in df_train.columns]

# 'X' will be all our features (the student's data), and 'y' will be our target (the stream they chose).
X_train = df_train[ALL_FEATURES]
y_train_raw = df_train[TARGET_COLUMN]
X_test = df_test[ALL_FEATURES]
y_test_raw = df_test[TARGET_COLUMN]

# Step 3: Teaching the Machine to Understand the Target
# The model needs numbers, not words like 'Science' or 'Arts'.
# Convert stream names (Science, Commerce, Arts) into numerical labels (0, 1, 2)
target_encoder = LabelEncoder()
y_train = target_encoder.fit_transform(y_train_raw)
y_test = target_encoder.transform(y_test_raw)

# Store the mapping for later interpretation
stream_mapping = {index: label for index, label in enumerate(target_encoder.classes_)}
print(f"\nI've converted the stream names to numbers. Here's the key: {stream_mapping}") # e.g., {0: 'Arts', 1: 'Commerce', 2: 'Science'}

# 4. Preprocess Features (Encoding Categorical & Scaling Numerical)
# We use ColumnTransformer to apply different transformations to different columns.
# OneHotEncoder for nominal categorical features.
# StandardScaler for numerical features.
# Hobby features are assumed binary (0/1), so 'passthrough' or scaling could be used. We'll scale them too.
numerical_features_for_scaling = NUMERICAL_FEATURES + HOBBY_FEATURES

# The ColumnTransformer is awesome for this. It lets me apply different steps to different columns all at once.
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features_for_scaling), # Scale all numerical data
        ('cat', OneHotEncoder(handle_unknown='ignore'), CATEGORICAL_FEATURES) # Convert categories to binary columns
    ],
    remainder='passthrough' # Keep any columns not specified (shouldn't be any if ALL_FEATURES is correct)
)

# Fit the preprocessor on the training data and transform both train and test data
print("\nTeaching my preprocessor the ropes using the training data...")
X_train_processed = preprocessor.fit_transform(X_train)
print("Applying the new rules to the training and testing data...")
X_test_processed = preprocessor.transform(X_test)

# Saving these fitted objects is a pro-move. It means I can use the exact same preprocessing on new data later.
joblib.dump(preprocessor, 'preprocessor.joblib')
joblib.dump(target_encoder, 'target_encoder.joblib') #Save the target encoder
print("Preprocessor and target encoder have been saved for future use.")

print(f"\nAfter preprocessing, the training data has this shape: {X_train_processed.shape}")
print(f"And the testing data has this shape: {X_test_processed.shape}")

# Step 5: Training Our First Model
print("\nTime to Build an XGBoost Model!")

# Initialize the XGBoost Classifier for multi-class classification
# 'multi:softprob' means it will give me the probability for each stream.
# num_class: specifies the number of classes
initial_xgb = xgb.XGBClassifier(
    objective='multi:softprob',
    num_class=len(stream_mapping), # It needs to know how many streams there are.
    eval_metric='mlogloss', # A Log loss for multi-class probability evaluation
    use_label_encoder=False, # # Recommended to set False with recent XGBoost versions
    random_state=42, # For reproducibility, so I get the same results every time.
    tree_method='hist' # A faster method for building the model.
)

print("Training the first version of the model... this might take a moment.")
initial_xgb.fit(X_train_processed, y_train)
print("Initial training complete!")

# Step 6: Checking the First Model's Performance
print("\n Evaluating Our First Attempt")

# Let's get the model's predictions on the test data.
y_pred_proba_initial = initial_xgb.predict_proba(X_test_processed)
y_pred_initial = initial_xgb.predict(X_test_processed)

# Now for the report card.
accuracy_initial = accuracy_score(y_test, y_pred_initial)
logloss_initial = log_loss(y_test, y_pred_proba_initial)
report_initial = classification_report(y_test, y_pred_initial, target_names=stream_mapping.values())

print(f"Initial Model Accuracy: {accuracy_initial:.4f}")
print(f"Initial Model Log Loss: {logloss_initial:.4f} (lower is better!)")
print("\nHere's the detailed breakdown:")
print(report_initial)

# Step 7: Fine-Tuning the Model (Hyperparameter Tuning)
print("\n Let's Try to Make the Model Even Better!")

# The default settings are okay, but we can probably do better by tuning the model's "hyperparameters".
# It's like adjusting the knobs on a stereo to get the best sound.

param_dist = {
    'n_estimators': [100, 200, 300, 400, 500, 700, 1000],
    'learning_rate': [0.001, 0.01, 0.05, 0.1, 0.2],
    'max_depth': [3, 5, 7, 9, 11],
    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],
    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],
    'gamma': [0, 0.1, 0.2, 0.3, 0.5, 1],
    'reg_alpha': [0, 0.001, 0.01, 0.1, 1],
    'reg_lambda': [0, 0.01, 0.1, 1, 10],
    'min_child_weight': [1, 3, 5, 7],
    'scale_pos_weight': [1, 3, 5]
}

# I'll use RandomizedSearchCV. It smartly samples a few combinations from the list above
# instead of trying every single one, which would take forever.
random_search = RandomizedSearchCV(
    estimator=xgb.XGBClassifier(
        tree_method='hist',
        objective='multi:softprob',
        num_class=len(stream_mapping),
        eval_metric='mlogloss',
        use_label_encoder=False,
        random_state=42
    ),
    param_distributions=param_dist,
    n_iter=20,          # How many different combinations to try.
    scoring='neg_log_loss',
    cv=3,               # Cross-validation folds for robust testing.
    verbose=2,          # This will show the progress.
    random_state=42,
    n_jobs=-1           # Use all available CPU cores to speed things up
)

print("Starting the search for the best model settings. This is the most intense part!")
random_search.fit(X_train_processed, y_train)

print("\nWoohoo! The search is complete.")
print(f"The best settings I found are: {random_search.best_params_}")
print(f"The best score I got during the search was: {random_search.best_score_:.4f}")

# The search automatically gives us the best model it found.
best_xgb_model = random_search.best_estimator_
print("\nOur final, tuned model is now ready to shine.")

#  Step 8: Evaluating the Final, Tuned Model
print("\n Final Model Evaluation")

# Let's see how this new-and-improved model does on the test data.
y_pred_proba_final = best_xgb_model.predict_proba(X_test_processed)
y_pred_final = best_xgb_model.predict(X_test_processed)


accuracy_final = accuracy_score(y_test, y_pred_final)
logloss_final = log_loss(y_test, y_pred_proba_final)
report_final = classification_report(y_test, y_pred_final, target_names=stream_mapping.values())

print(f"Tuned Model Accuracy: {accuracy_final:.4f}")
print(f"Tuned Model Log Loss: {logloss_final:.4f}")
print("\nFinal Tuned Model Classification Report:")
print(report_final)

# Let's see the improvement side-by-side.
print("\n--- How much better did we get? ---")
print(f"Accuracy: {accuracy_initial:.4f} (Initial)  ->  {accuracy_final:.4f} (Tuned)")
print(f"Log Loss: {logloss_initial:.4f} (Initial)  ->  {logloss_final:.4f} (Tuned)")

#  Step 9: Adding Our Predictions Back to the Data
print("\n Finalizing the Datasets with Predictions")

# Let's add the predicted probabilities for each stream
# as new columns in our original datasets.
print("Predicting probabilities for both training and testing sets...")
train_probabilities = best_xgb_model.predict_proba(X_train_processed)
test_probabilities = y_pred_proba_final # We already calculated this one!

# Let's create the new columns.
probability_columns = []
for i, stream_name in stream_mapping.items():
    col_name = f'Prob_{stream_name}'
    probability_columns.append(col_name)
    df_train_orig[col_name] = train_probabilities[:, i]
    df_test_orig[col_name] = test_probabilities[:, i]

print("\nLet's check if the new probability columns were added...")
print("Training DataFrame Columns:", df_train_orig.columns.tolist())
print("Testing DataFrame Columns:", df_test_orig.columns.tolist())

# And one last look at our final product.
print("\nHere's how the training data looks with the new probabilities:")
display_cols_train = df_train.columns.tolist() + probability_columns
print(df_train_orig[display_cols_train].head())

print("\nAnd the testing data:")
display_cols_test = df_test.columns.tolist() + probability_columns
print(df_test_orig[display_cols_test].head())

# Step 10: Saving our progress
# Let's save these datasets to new CSV files.
try:
    output_train_file = 'dataset_1_with_predictions.csv'
    output_test_file = 'dataset_2_with_predictions.csv'
    df_train_orig.to_csv(output_train_file, index=False)
    df_test_orig.to_csv(output_test_file, index=False)
    print(f"\nSuccessfully saved the updated training data to '{output_train_file}'")
    print(f"Successfully saved the updated testing data to '{output_test_file}'")
except Exception as e:
    print(f"\n Something went wrong while saving the files: {e}")

print("\nAll done! The datasets now include the model's predictions.")

# This function takes a new student's data,
# preprocesses it just like we did before, and uses our trained model to predict
# the stream probabilities.

def predict_new_student(student_data, model, preprocessor, target_encoder):
    """
    Takes a dictionary of a new student's details, processes them, and predicts stream probabilities.

    Args:
        student_data (dict): A dictionary with all the student's info. The keys must match the feature names!
        model: Our final, tuned XGBoost model (best_xgb_model).
        preprocessor: The preprocessor we fitted on our training data.
        target_encoder: The target encoder we used for the stream names.

    Returns:
        dict: A dictionary mapping stream names to their predicted probabilities (e.g., {'Science': 0.8, ...}).
              Returns None if something goes wrong.
    """
    print("\n Preparing the new student's data for the model...")
    try:
        # The preprocessor expects a DataFrame, so I'll convert the dictionary.
        # It's crucial that the column order matches the original 'ALL_FEATURES' list.
        student_df = pd.DataFrame([student_data])[ALL_FEATURES]
    except KeyError as e:
        print(f"Error: Looks like we're missing a feature in the input data: {e}")
        return None
    except Exception as e:
        print(f"Error creating the DataFrame for the new student: {e}")
        return None

    # Use the pre-fitted preprocessor to transform the new data.
    try:
        student_processed = preprocessor.transform(student_df)
        print("Preprocessing successful.")
    except Exception as e:
        print(f"An error occurred while preprocessing the new data: {e}")
        return None

    # Time for the magic! Get the probabilities from the model.
    try:
        print("Making the prediction...")
        probabilities = model.predict_proba(student_processed)[0] # We only have one student, so we take the first result.
        print("Prediction complete!")
    except Exception as e:
        print(f"The model ran into an error during prediction: {e}")
        return None

    # Finally, let's map these probabilities back to the readable stream names.
    try:
        stream_probabilities = {}
        class_names = target_encoder.classes_
        for i, prob in enumerate(probabilities):
            stream_name = class_names[i]
            stream_probabilities[stream_name] = prob
        return stream_probabilities
    except Exception as e:
        print(f"Something went wrong when mapping probabilities to stream names: {e}")
        return None

#  Interactive Part: Predict for a New Student!
print("\n\n Let's Predict for a New Student! ")

# A helper function to make sure we get valid numbers for marks.
def get_numeric_input(prompt):
    while True:
        try:
            value = float(input(prompt))
            if 0 <= value <= 100:
                return value
            else:
                print("Hmm, that doesn't look right. Please enter a value between 0 and 100.")
        except ValueError:
            print("That's not a number! Please try again.")

# This function will guide the user through entering all the student's details.
def get_student_details_interactive():
    details = {}
    print("Please provide the following details for the student:")

    # Basic Info
    details['gender'] = input("Gender (e.g., Male, Female): ").strip().title()
    details['family_background'] = input("Family Background (e.g., Doctor, Engineering, Business): ").strip().title()

    # Hobbies (collecting yes/no and converting to 1/0)
    print("\nNow for hobbies. Just enter 'yes' or 'no'.")
    hobby_list = HOBBY_FEATURES # Using the list from our config
    for hobby in hobby_list:
        while True:
            response = input(f"  Is the student interested in {hobby.replace('hobby_', '').replace('_', ' ')}? (yes/no): ").lower()
            if response in ['yes', 'y']:
                details[hobby] = 1
                break
            elif response in ['no', 'n']:
                details[hobby] = 0
                break
            else:
                print("Invalid input. Please enter 'yes' or 'no'.")

    # Marks from Grade 5 to 10
    print("\nGreat! Now let's get the academic marks (from 0-100).")
    subjects = ['math', 'science', 'social_science', 'english', 'second_language']
    for grade in range(5, 11):
        print(f"\nEnter Marks for Grade {grade}:")
        for subject in subjects:
            feature_name = f"{subject}_{grade}"
            details[feature_name] = get_numeric_input(f"  {subject.replace('_', ' ').title()}: ")

    print("\nThanks! I have all the basic details.")
    return details

# This function calculates the extra features like averages and growth metrics.
def calculate_derived_features(details):
    print("\nNow, I'll calculate some extra features based on the marks provided...")
    calculated_details = details.copy()
    subjects = ['math', 'science', 'social_science', 'english', 'second_language']

    # Calculate Averages (Grade 8-10)
    for subject in subjects:
        try:
            avg_key = f"avg_{subject}_8_10"
            marks_8_10 = [calculated_details[f"{subject}_8"], calculated_details[f"{subject}_9"], calculated_details[f"{subject}_10"]]
            calculated_details[avg_key] = sum(marks_8_10) / 3.0
            print(f"  Calculated average for {subject.replace('_', ' ')} (Grades 8-10): {calculated_details[avg_key]:.2f}")
        except KeyError as e:
            print(f"  Warning: Couldn't calculate average for {subject} because a mark was missing ({e}). Setting to 0.")
            calculated_details[avg_key] = 0

    # Calculate Science Growth
    try:
        calculated_details['science_growth'] = calculated_details['science_10'] - calculated_details['science_8']
        print(f"  Calculated Science Growth (Grade 10 - Grade 8): {calculated_details['science_growth']:.2f}")
    except KeyError as e:
        print(f"  Warning: Couldn't calculate Science Growth. Missing marks ({e}). Setting to 0.")
        calculated_details['science_growth'] = 0

    # Calculate Consistency Growth
    try:
        marks_10 = [calculated_details[f"{subj}_10"] for subj in subjects]
        marks_8 = [calculated_details[f"{subj}_8"] for subj in subjects]
        avg_marks_10 = sum(marks_10) / len(marks_10)
        avg_marks_8 = sum(marks_8) / len(marks_8)
        calculated_details['consistency_growth'] = avg_marks_10 - avg_marks_8
        print(f"  Calculated Consistency Growth (Avg Grade 10 - Avg Grade 8): {calculated_details['consistency_growth']:.2f}")
    except (KeyError, ZeroDivisionError) as e:
        print(f"  Warning: Couldn't calculate Consistency Growth ({e}). Setting to 0.")
        calculated_details['consistency_growth'] = 0

    print("All derived features calculated!")
    return calculated_details

# --- The Main Interactive Flow ---
# Let's put it all together.

# 1. Get the student's details from the user.
user_input_details = get_student_details_interactive()

# 2. Calculate the derived features.
all_student_features = calculate_derived_features(user_input_details)

# 3. Last check to make sure we have everything we need for the model.
missing_for_prediction = [f for f in ALL_FEATURES if f not in all_student_features]
if missing_for_prediction:
    print(f"\nError: I can't make a prediction because these features are missing: {missing_for_prediction}")
else:
    # 4. If everything is in order, let's make the prediction!
    try:
        # Let's make sure the necessary prediction tools are loaded and ready.
        # They should be in memory if you ran the whole script, but this is a good safety check.
        if 'best_xgb_model' not in globals() or 'preprocessor' not in globals() or 'target_encoder' not in globals():
             print("\nLoading the saved model and preprocessors...")
             best_xgb_model = joblib.load('best_xgb_model.joblib') # You'd need to save the model itself for this to work
             preprocessor = joblib.load('preprocessor.joblib')
             target_encoder = joblib.load('target_encoder.joblib')

        # Calling our prediction function!
        predicted_probs = predict_new_student(all_student_features, best_xgb_model, preprocessor, target_encoder)

        if predicted_probs:
            print("\n--- Here are the Predicted Stream Probabilities ---")
            # I'll sort the results to show the most likely stream first.
            sorted_probs = sorted(predicted_probs.items(), key=lambda item: item[1], reverse=True)
            for stream, prob in sorted_probs:
                print(f"  {stream}: {prob*100:.2f}%")

            most_likely_stream = sorted_probs[0][0]
            print(f"\nBased on this, the student is most likely to choose: **{most_likely_stream}** 🎉")
        else:
            print("\nPrediction failed. Please review the inputs and any error messages above.")

    except FileNotFoundError:
        print("\nError! I couldn't find the necessary saved files ('preprocessor.joblib', 'target_encoder.joblib').")
        print("Please make sure you've run the training part of the script first.")
    except Exception as e:
        print(f"\nAn unexpected error happened during the final prediction step: {e}")